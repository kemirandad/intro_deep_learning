{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this lesson we're going to see how we can build neural networks capable of learning the complex kinds of relationships deep neural nets are famous for.\n",
    "\n",
    "The key idea here is _modularity_, building up a complex network from simpler functional units. We've seen how a linear unit computes a linear function -- now we'll see how to combine and modify these single units to model more complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "Neural networks typically organize their neurons into **layers**. When we collect together linear units having a common set of inputs we get a **dense** layer.\n",
    "\n",
    "![img](https://i.imgur.com/2MA4iMV.png)\n",
    "\n",
    "_A dense layer of two linear units receiving two inputs and a bias._\n",
    "\n",
    "You could think of each layer in a neural network as performing some kind of relatively simple transformation. Through a deep stack of layers, a neural network can transform its inputs in more and more complex ways. In a well-trained neural network, each layer is a transformation getting us a little bit closer to a solution.\n",
    "\n",
    "> **Many Kinds of Layers**\n",
    "    A \"layer\" in Keras is a very general kind of thing. A layer can be, essentially, any kind of _data transformation_. Many layers, like the [convolutional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [recurrent](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN) layers, transform data through use of neurons and differ primarily in the pattern of connections they form. Others though are used for [feature engineering](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) or just [simple arithmetic](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add). There's a whole world of layers to discover -- [check them out](https://www.tensorflow.org/api_docs/python/tf/keras/layers)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Activation Function\n",
    "It turns out, however, that two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something _nonlinear_. What we need are activation functions.\n",
    "\n",
    "![img](https://i.imgur.com/OLSUEYT.png)\n",
    "\n",
    "_Without activation functions, neural networks can only learn linear relationships. In order to fit curves, we'll need to use activation functions._\n",
    "\n",
    "An **activation function** is simply some function we apply to each of a layer's outputs (its _activations_). The most common is the _rectifier_ function  $max(0,x)$\n",
    "\n",
    "![img](https://i.imgur.com/aeIyAlF.png)\n",
    "\n",
    "The rectifier function has a graph that's a line with the negative part \"rectified\" to zero. Applying the function to the outputs of a neuron will put a _bend_ in the data, moving us away from simple lines.\n",
    "\n",
    "When we attach the rectifier to a linear unit, we get a **rectified linear unit** or **ReLU**. (For this reason, it's common to call the rectifier function the \"ReLU function\".) Applying a ReLU activation to a linear unit means the output becomes `max(0, w * x + b)`, which we might draw in a diagram like:\n",
    "\n",
    "![img](https://i.imgur.com/eFry7Yu.png)\n",
    "\n",
    "_A rectified linear unit_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Dense Layers\n",
    "Now that we have some nonlinearity, let's see how we can stack layers to get complex data transformations.\n",
    "\n",
    "![img](https://i.imgur.com/Y5iwFQZ.png)\n",
    "\n",
    "_A stack of dense layers makes a \"fully-connected\" network_\n",
    "\n",
    "The layers before the output layer are sometimes called **hidden** since we never see their outputs directly.\n",
    "\n",
    "Now, notice that the final (output) layer is a linear unit (meaning, no activation function). That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output.\n",
    "\n",
    "### Building Sequential Models\n",
    "The `Sequential` model we've been using will connect together a list of layers in order from first to last: the first layer gets the input, the last layer produces the output. This creates the model in the figure above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 15:50:41.815800: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-05 15:50:41.816902: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-05 15:50:41.820461: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to pass all the layers together in a list, like `[layer, layer, layer, ...]`, instead of as separate arguments. To add an activation function to a layer, just give its name in the `activation` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98ebd6681bbe0e113db900b62dbac20f4991abc4ff6872347a03acc8ab2ec634"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('deep_learn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
